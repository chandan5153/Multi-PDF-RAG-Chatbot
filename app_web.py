# âœ… app_web.py - Streamlit Multi-PDF RAG Bot

import os
import fitz  # PyMuPDF
import streamlit as st
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
from langchain_community.llms import Ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_core.runnables import RunnableConfig

# âœ… Load environment
load_dotenv()
model_name = os.getenv("LLM_MODEL", "llama3")

# âœ… UI setup
st.set_page_config(page_title="Multi-PDF RAG Bot", layout="wide")
st.title("ğŸ“„ Multi-PDF RAG Chatbot")

# âœ… Load PDFs and extract text
@st.cache_resource
def load_pdf_text():
    pdf_folder = "data"
    pdf_files = ["resume.pdf"]
    all_text = ""
    for file_name in pdf_files:
        file_path = os.path.join(pdf_folder, file_name)
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"âŒ File not found: {file_path}")
        doc = fitz.open(file_path)
        text = "\n".join(page.get_text() for page in doc)
        all_text += f"\n--- Start of {file_name} ---\n{text}\n--- End of {file_name} ---\n"
    return all_text

# âœ… Vectorstore setup
@st.cache_resource
def setup_rag(all_text):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.create_documents([all_text])
    if not docs:
        raise ValueError("âŒ No text chunks generated.")
    embedding = OllamaEmbeddings(model=model_name)
    vectorstore = FAISS.from_documents(docs, embedding)
    retriever = vectorstore.as_retriever()
    llm = Ollama(model=model_name)
    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# âœ… Initialize
with st.spinner("ğŸ” Processing PDF and loading model..."):
    all_text = load_pdf_text()
    rqa = setup_rag(all_text)

# âœ… Chat UI
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

query = st.text_input("ğŸ’¬ Ask your question:")
if query:
    with st.spinner("ğŸ¤– Thinking..."):
        result = rqa.invoke(query, config=RunnableConfig())
        st.session_state.chat_history.append(("You", query))
        st.session_state.chat_history.append(("Bot", result["result"]))

# âœ… Display chat
for sender, message in st.session_state.chat_history:
    if sender == "You":
        st.markdown(f"**ğŸ§‘â€ğŸ’» You:** {message}")
    else:
        st.markdown(f"**ğŸ¤– Bot:** {message}")
